{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec, StringIndexer, IndexToString, StopWordsRemover\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/home/hadi/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Distributed Computing/Without PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_29981/2999134276.py:28: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_undistributed['review'][i] = ' '.join(filtered_sentence)\n",
      "/tmp/ipykernel_29981/2999134276.py:29: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  df_undistributed['review'][i] = df_undistributed['review'][i].lower()\n"
     ]
    }
   ],
   "source": [
    "df_undistributed=pd.read_csv('IMDB Dataset.csv')\n",
    "df_undsitributed_train=df_undistributed.iloc[:40000,:]\n",
    "df_undsitributed_test=df_undistributed.iloc[10000:,:]\n",
    "\n",
    "\n",
    "words=[',','<','>','?','br','/','.','-',';',':','(',')','[',']','{','}','!','@','#','$','%','^','&','*','_','+','=','|','\\\\','\"',\"'\",'<br,','<br','<br>','<br/>','<br />','<br >','<br /']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('.')\n",
    "stop_words.add('<')\n",
    "stop_words.add('>')\n",
    "stop_words.add('?')\n",
    "stop_words.add('br')\n",
    "stop_words.add('/')\n",
    "stop_words.add('.')\n",
    "stop_words.add('-')\n",
    "stop_words.add(';')\n",
    "\n",
    "for i in range(len(df_undistributed['review'])):\n",
    "\tword_tokens = word_tokenize(df_undistributed['review'][i])\n",
    "\tfiltered_sentence = [w for w in word_tokens if not w in stop_words and w != ',']\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in word_tokens:\n",
    "\t\tif w not in stop_words and w != ',':\n",
    "\t\t\tlemmatizer.lemmatize(w)\n",
    "\t\t\tfiltered_sentence.append(w)\n",
    "\tdf_undistributed['review'][i] = ' '.join(filtered_sentence)\n",
    "\tdf_undistributed['review'][i] = df_undistributed['review'][i].lower()\n",
    "\n",
    "\n",
    "tokenized_reviews = [word_tokenize(review) for review in df_undistributed['review']]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7534\n",
      "F1: 0.7533951661757595\n",
      "['positive']\n",
      "[[0.43786767 0.56213233]]\n",
      "['positive']\n",
      "[[0.451275 0.548725]]\n",
      "['positive']\n",
      "[[0.3066833 0.6933167]]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Train Word2Vec model\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=30, window=5, min_count=1, workers=4)\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)  \n",
    "\n",
    "X = [document_vector(word2vec_model, doc) for doc in tokenized_reviews]\n",
    "y = df_undistributed['sentiment']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=4)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Predict and Evaluate\n",
    "y_pred = RF.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test, y_pred, average='weighted'))\n",
    "\n",
    "# Test with sample text\n",
    "text = 'I love this movie, But the title is not good overall the movie was lovely'\n",
    "text = word_tokenize(text)\n",
    "text = [document_vector(word2vec_model, text)]\n",
    "print(RF.predict(text))\n",
    "print(RF.predict_proba(text))\n",
    "\n",
    "text_2 = 'I hate this movie'\n",
    "text_2 = word_tokenize(text_2)\n",
    "text_2 = [document_vector(word2vec_model, text_2)]\n",
    "print(RF.predict(text_2))\n",
    "print(RF.predict_proba(text_2))\n",
    "\n",
    "text_3 = 'The movie was very strong and powerful'\n",
    "text_3 = word_tokenize(text_3)\n",
    "text_3 = [document_vector(word2vec_model, text_3)]\n",
    "print(RF.predict(text_3))\n",
    "print(RF.predict_proba(text_3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISTRIBUTED WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"spark://172.28.111.195:7077\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()\n",
    "df_train=df[:20000]\n",
    "df_test=df[20000:]\n",
    "spark_df=spark.createDataFrame(df)\n",
    "spark_df_train,spark_df_test=spark_df.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 22:07:44 WARN TaskSetManager: Stage 78 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:07:46 WARN TaskSetManager: Stage 81 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:08:00 WARN TaskSetManager: Stage 83 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:09:42 WARN TaskSetManager: Stage 86 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:09:46 WARN TaskSetManager: Stage 87 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:09:51 WARN TaskSetManager: Stage 88 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:09:55 WARN TaskSetManager: Stage 90 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:10:00 WARN TaskSetManager: Stage 92 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:10:01 WARN TaskSetManager: Stage 94 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:10:03 WARN TaskSetManager: Stage 96 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/05/01 22:10:08 WARN TaskSetManager: Stage 98 contains a task of very large size (8079 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6688687874784023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/01 22:10:16 WARN StringIndexerModel: Input column sentiment does not exist during transformation. Skip StringIndexerModel for this column.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "|              review|               words|            filtered|            features|       rawPrediction|         probability|prediction|predicted_sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "|I love this movie...|[i, love, this, m...|[i, love, this, m...|[0.01370990922053...|[57.0238147530620...|[0.57023814753062...|       0.0|           negative|\n",
      "|   I hate this movie|[i, hate, this, m...|[i, hate, this, m...|[0.10460961423814...|[63.8305225552969...|[0.63830522555296...|       0.0|           negative|\n",
      "|The movie was ver...|[the, movie, was,...|[the, movie, was,...|[-0.0372891569776...|[42.2267492494330...|[0.42226749249433...|       1.0|           positive|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, Word2Vec\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import StringIndexer, IndexToString\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Define stopwords and tokenizer\n",
    "words = [',', '<', '>', '?', 'br', '/', '.', '-', ';', ':', '(', ')', '[', ']', '{', '}', '!', '@', '#', '$', '%', '^', '&', '*', '_', '+', '=', '|', '\\\\', '\"', \"'\", '<br,', '<br', '<br>', '<br/>', '<br />', '<br >', '<br /']\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "\n",
    "# Define remover for stopwords\n",
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=words)\n",
    "\n",
    "# Define Word2Vec model\n",
    "word2vec = Word2Vec(vectorSize=30, minCount=1, windowSize=5, inputCol=remover.getOutputCol(), outputCol=\"features\")\n",
    "\n",
    "# Define label indexer\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\").fit(spark_df_train)\n",
    "\n",
    "# Define Random Forest classifier\n",
    "rf = RandomForestClassifier(numTrees=100, maxDepth=4, labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Define label converter\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_sentiment\", labels=labelIndexer.labels)\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, word2vec, labelIndexer, rf, labelConverter])\n",
    "\n",
    "# Train the pipeline\n",
    "model = pipeline.fit(spark_df_train)\n",
    "\n",
    "# Make predictions\n",
    "prediction = model.transform(spark_df_test)\n",
    "\n",
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Test with sample text\n",
    "text_spark = 'I love this movie, But the title is not good overall the movie was lovely'\n",
    "text_spark_2 = 'I hate this movie'\n",
    "text_spark_3 = 'The movie was very strong and powerful'\n",
    "\n",
    "text_data = [text_spark, text_spark_2, text_spark_3]\n",
    "text_data = pd.DataFrame(text_data, columns=['review'])\n",
    "text_data = spark.createDataFrame(text_data)\n",
    "\n",
    "prediction = model.transform(text_data)\n",
    "prediction.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
