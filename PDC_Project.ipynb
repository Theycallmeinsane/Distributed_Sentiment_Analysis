{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hadi/.local/lib/python3.10/site-packages/pyspark/pandas/__init__.py:50: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import PorterStemmer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import IDF\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.ml.feature import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import Tokenizer, Word2Vec, StringIndexer, IndexToString, StopWordsRemover\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator,BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, TrainValidationSplit\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()\n",
    "os.chdir('/home/hadi/Datasets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without Distributed Computing/Without PySpark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_undistributed=pd.read_csv('IMDB Dataset.csv')\n",
    "df_undsitributed_train=df_undistributed.iloc[:40000,:]\n",
    "df_undsitributed_test=df_undistributed.iloc[10000:,:]\n",
    "\n",
    "\n",
    "words=[',','<','>','?','br','/','.','-',';',':','(',')','[',']','{','}','!','@','#','$','%','^','&','*','_','+','=','|','\\\\','\"',\"'\",'<br,','<br','<br>','<br/>','<br />','<br >','<br /']\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add('.')\n",
    "stop_words.add('<')\n",
    "stop_words.add('>')\n",
    "stop_words.add('?')\n",
    "stop_words.add('br')\n",
    "stop_words.add('/')\n",
    "stop_words.add('.')\n",
    "stop_words.add('-')\n",
    "stop_words.add(';')\n",
    "\n",
    "for i in range(len(df_undistributed['review'])):\n",
    "\tword_tokens = word_tokenize(df_undistributed['review'][i])\n",
    "\tfiltered_sentence = [w for w in word_tokens if not w in stop_words and w != ',']\n",
    "\tfiltered_sentence = []\n",
    "\tfor w in word_tokens:\n",
    "\t\tif w not in stop_words and w != ',':\n",
    "\t\t\tlemmatizer.lemmatize(w)\n",
    "\t\t\tfiltered_sentence.append(w)\n",
    "\tdf_undistributed['review'][i] = ' '.join(filtered_sentence)\n",
    "\tdf_undistributed['review'][i] = df_undistributed['review'][i].lower()\n",
    "\n",
    "\n",
    "tokenized_reviews = [word_tokenize(review) for review in df_undistributed['review']]\n",
    "word2vec_model = Word2Vec(sentences=tokenized_reviews, vector_size=10, window=5, min_count=1, workers=4)\n",
    "\n",
    "print(word2vec_model)\n",
    "\n",
    "\n",
    "\n",
    "def document_vector(word2vec_model, doc):\n",
    "    doc = [word for word in doc if word in word2vec_model.wv.key_to_index]\n",
    "    if len(doc) == 0:\n",
    "        return np.zeros(word2vec_model.vector_size)\n",
    "    return np.mean(word2vec_model.wv[doc], axis=0)  \n",
    "\n",
    "X = [document_vector(word2vec_model, doc) for doc in tokenized_reviews]\n",
    "y = df_undistributed['sentiment']\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 4: No need to vectorize data, as we already have numerical vectors\n",
    "\n",
    "# Step 5: Train RandomForestClassifier\n",
    "RF = RandomForestClassifier(max_depth=4)\n",
    "RF.fit(X_train, y_train)\n",
    "\n",
    "# Step 6: Predict and Evaluate\n",
    "y_pred = RF.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, y_pred))\n",
    "print('F1:', f1_score(y_test, y_pred, average='weighted'))\n",
    "text='I love this movie, But the title is not good overall the movie was lovely'\n",
    "text=word_tokenize(text)\n",
    "text=[document_vector(word2vec_model, text)]\n",
    "print(RF.predict(text))\n",
    "print(RF.predict_proba(text))\n",
    "\n",
    "text_2='I hate this movie'\n",
    "text_2=word_tokenize(text_2)\n",
    "text_2=[document_vector(word2vec_model, text_2)]\n",
    "print(RF.predict(text_2))\n",
    "print(RF.predict_proba(text_2))\n",
    "\n",
    "text_3='The movie was very strong and powerful'\n",
    "text_3=word_tokenize(text_3)\n",
    "text_3=[document_vector(word2vec_model, text_3)]\n",
    "print(RF.predict(text_3))\n",
    "print(RF.predict_proba(text_3))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DISTRIBUTED WORD EMBEDDINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder \\\n",
    "    .appName(\"Spark NLP\") \\\n",
    "    .master(\"spark://172.28.111.195:4040\") \\\n",
    "    .config(\"spark.driver.memory\", \"16G\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryoserializer.buffer.max\", \"2000M\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"0\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.johnsnowlabs.nlp:spark-nlp_2.12:5.3.3\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('IMDB Dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df[:20000]\n",
    "df_test=df[20000:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive\n",
       "2  I thought this was a wonderful way to spend ti...  positive\n",
       "3  Basically there's a family where a little boy ...  negative\n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...  positive"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  One of the other reviewers has mentioned that ...  positive\n",
      "1  A wonderful little production. <br /><br />The...  positive\n",
      "2  I thought this was a wonderful way to spend ti...  positive\n",
      "3  Basically there's a family where a little boy ...  negative\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df=spark.createDataFrame(df)\n",
    "spark_df_train,spark_df_test=spark_df.randomSplit([0.8,0.2],seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark_df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 11:51:20 WARN TaskSetManager: Stage 0 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:51:27 WARN TaskSetManager: Stage 3 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:51:38 WARN TaskSetManager: Stage 5 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:51:44 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "24/04/24 11:52:39 WARN TaskSetManager: Stage 8 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:39 WARN TaskSetManager: Stage 9 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:42 WARN TaskSetManager: Stage 10 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:45 WARN TaskSetManager: Stage 12 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:48 WARN TaskSetManager: Stage 14 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:50 WARN TaskSetManager: Stage 16 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:51 WARN TaskSetManager: Stage 18 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "24/04/24 11:52:54 WARN TaskSetManager: Stage 20 contains a task of very large size (8078 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 20:===========================================>              (6 + 2) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6237422502286818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "words=[',','<','>','?','br','/','.','-',';',':','(',')','[',']','{','}','!','@','#','$','%','^','&','*','_','+','=','|','\\\\','\"',\"'\",'<br,','<br','<br>','<br/>','<br />','<br >','<br /']\n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"review\", outputCol=\"words\")\n",
    "remover=StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\", stopWords=words)\n",
    "word2vec = Word2Vec(vectorSize=10, windowSize=5, inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n",
    "labelIndexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\").fit(spark_df_train)\n",
    "rf = RandomForestClassifier(numTrees=100, maxDepth=4, labelCol=\"label\", featuresCol=\"features\")\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predicted_sentiment\", labels=labelIndexer.labels)\n",
    "\n",
    "\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer,remover, word2vec, labelIndexer, rf, labelConverter])\n",
    "model = pipeline.fit(spark_df_train)\n",
    "prediction = model.transform(spark_df_test)\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'prediction'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator \u001b[38;5;241m=\u001b[39m \u001b[43mBinaryClassificationEvaluator\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabelCol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetricName\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maccuracy\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(prediction)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/__init__.py:139\u001b[0m, in \u001b[0;36mkeyword_only.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMethod \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m forces keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_input_kwargs \u001b[38;5;241m=\u001b[39m kwargs\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: BinaryClassificationEvaluator.__init__() got an unexpected keyword argument 'prediction'"
     ]
    }
   ],
   "source": [
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", prediction=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(prediction)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/24 11:54:26 WARN StringIndexerModel: Input column sentiment does not exist during transformation. Skip StringIndexerModel for this column.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "|              review|               words|            filtered|            features|       rawPrediction|         probability|prediction|predicted_sentiment|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "|I love this movie...|[i, love, this, m...|[i, love, this, m...|[-0.3091462463140...|[45.3715566107725...|[0.45371556610772...|       1.0|           positive|\n",
      "|   I hate this movie|[i, hate, this, m...|[i, hate, this, m...|[-0.5255057960748...|[54.3145256390977...|[0.54314525639097...|       0.0|           negative|\n",
      "|The movie was ver...|[the, movie, was,...|[the, movie, was,...|[-0.1830796450376...|[37.9402120425197...|[0.37940212042519...|       1.0|           positive|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_spark='I love this movie, But the title is not good overall the movie was lovely'\n",
    "text_spark_2='I hate this movie'\n",
    "text_spark_3='The movie was very strong and powerful'\n",
    "\n",
    "text_data=[text_spark,text_spark_2,text_spark_3]\n",
    "text_data=pd.DataFrame(text_data,columns=['review'])\n",
    "text_data=spark.createDataFrame(text_data)\n",
    "\n",
    "prediction=model.transform(text_data)\n",
    "prediction.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
